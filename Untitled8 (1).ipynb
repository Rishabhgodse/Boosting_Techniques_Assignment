{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nn5n0-UzDTVe"
      },
      "outputs": [],
      "source": [
        "# 1. What is Boosting in Machine Learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting is an ensemble learning technique that combines multiple weak learners (usually decision trees) to create a strong learner. The idea is to sequentially train models, where each model tries to correct the errors made by the previous one. Boosting aims to reduce both bias and variance in predictions."
      ],
      "metadata": {
        "id": "2qyzYxxRH6da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. How does Boosting differ from Bagging?"
      ],
      "metadata": {
        "id": "BbToydiADVno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While both are ensemble methods, Boosting trains models sequentially, with each model focusing on the mistakes of the previous ones. In contrast, Bagging trains models independently and then aggregates their predictions. Bagging reduces variance by averaging predictions, while Boosting reduces bias by improving model accuracy step by step."
      ],
      "metadata": {
        "id": "MeGFcvgSH-ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. What is the key idea behind AdaBoost?"
      ],
      "metadata": {
        "id": "ewadfVc-DVkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AdaBoost (Adaptive Boosting) is a popular Boosting algorithm that adjusts the weights of misclassified instances, forcing the next weak learner to focus more on these difficult cases. The idea is to adaptively combine weak models into a strong one."
      ],
      "metadata": {
        "id": "hpR2fD2HICJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Explain the working of AdaBoost with an example."
      ],
      "metadata": {
        "id": "t_aeoZshDVh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In AdaBoost, a base learner (e.g., a decision tree stump) is trained on the dataset, and misclassified instances are given higher weights. The process is repeated, and each learner is assigned a weight based on its accuracy. For example, in a binary classification task, AdaBoost might first classify easy points, but after boosting, the model corrects itself by focusing more on points it previously misclassified."
      ],
      "metadata": {
        "id": "7ca8-AJpIGD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. What is Gradient Boosting, and how is it different from AdaBoost?"
      ],
      "metadata": {
        "id": "wtk07iqIDVfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradient Boosting builds models sequentially like AdaBoost, but instead of reweighting instances, it uses the gradient of the loss function to identify errors. Each model is trained to minimize the residual errors (difference between actual and predicted values) from the previous model, using gradient descent."
      ],
      "metadata": {
        "id": "hLv-k87zIJ_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. What is the loss function in Gradient Boosting?"
      ],
      "metadata": {
        "id": "tSgi8QoVDVcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The loss function in Gradient Boosting varies depending on the task. For regression, it's typically Mean Squared Error (MSE), while for classification, it could be Log Loss (cross-entropy). The model tries to minimize this loss by updating the weights based on the gradient of the error."
      ],
      "metadata": {
        "id": "uX8SPKz0IQE2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. How does XGBoost improve over traditional Gradient Boosting?"
      ],
      "metadata": {
        "id": "_CBajryPDVZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) introduces several optimizations, including:\n",
        "\n",
        "Regularization: Prevents overfitting by controlling model complexity.\n",
        "\n",
        "Tree Pruning: Stops the construction of trees when further splitting has no gain.\n",
        "\n",
        "Parallelization: Speeds up computation by parallelizing tasks.\n",
        "\n",
        "Handling Missing Data: Efficiently handles missing values by learning best imputation strategies."
      ],
      "metadata": {
        "id": "6E_Sep_hIW_a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. What is the difference between XGBoost and CatBoost?"
      ],
      "metadata": {
        "id": "rcP6peWbDVYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "While both are advanced boosting algorithms, XGBoost is highly efficient for numerical and sparse data, while CatBoost is designed to handle categorical features effectively without the need for preprocessing (like one-hot encoding). CatBoost reduces overfitting in high-cardinality categorical features."
      ],
      "metadata": {
        "id": "XC8UcOhzIfE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. What are some real-world applications of Boosting techniques?"
      ],
      "metadata": {
        "id": "w6JTBkxoDVVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Boosting techniques are used in a wide range of applications, such as:\n",
        "\n",
        "Fraud detection: Identifying fraudulent transactions.\n",
        "\n",
        "\n",
        "Marketing: Predicting customer churn or preferences.\n",
        "\n",
        "Finance: Credit scoring and risk analysis.\n",
        "\n",
        "Healthcare: Disease prediction and patient diagnosis."
      ],
      "metadata": {
        "id": "c5MmJNDnIizQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. How does regularization help in XGBoost?"
      ],
      "metadata": {
        "id": "A1AqkVAzDVSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regularization in XGBoost (L1 and L2) penalizes complex models, thus preventing overfitting. L1 regularization (lasso) shrinks coefficients to zero, simplifying the model, while L2 regularization (ridge) limits large weights to maintain a balanced model."
      ],
      "metadata": {
        "id": "XCEKXpQSIpsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. What are some hyperparameters to tune in Gradient Boosting models?"
      ],
      "metadata": {
        "id": "UptGtBfWDVPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key hyperparameters include:\n",
        "\n",
        "Learning Rate: Controls the contribution of each model.\n",
        "\n",
        "Number of Trees: Defines how many weak learners are used.\n",
        "\n",
        "Max Depth: Limits the complexity of trees.\n",
        "\n",
        "Subsample: Fraction of data used for training.\n",
        "\n",
        "Min Samples Split/Leaf: Limits the number of samples required to split a node."
      ],
      "metadata": {
        "id": "dyEPAgWiIrJl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. What is the concept of Feature Importance in Boosting?"
      ],
      "metadata": {
        "id": "OWTdeAaFDVM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Importance refers to the contribution of each feature in making predictions. In Boosting models, it is often calculated based on how often a feature is used to split a node and the resulting improvement in performance."
      ],
      "metadata": {
        "id": "oAFJI4NeIzX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Why is CatBoost efficient for categorical data?"
      ],
      "metadata": {
        "id": "CcvAL6mCDVKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CatBoost is efficient for categorical data because it uses Ordered Target Encoding, which preserves the integrity of the training data by avoiding information leakage. It also automates the handling of categorical variables without the need for explicit encoding, making it more robust for real-world datasets."
      ],
      "metadata": {
        "id": "0r89MK7pI2rJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "                                                                    # Practical"
      ],
      "metadata": {
        "id": "xsnWFMAuDVH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Train an AdaBoost Classifier on a sample dataset and print model accuracy"
      ],
      "metadata": {
        "id": "sz45-bwTDVFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a sample dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "adb_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "adb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = adb_clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "-pf3CP0YMWEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Train an AdaBoost Regressor and evaluate performance using Mean Absolute Error (MAE)"
      ],
      "metadata": {
        "id": "fBsaqSnqDVDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Regressor\n",
        "adb_reg = AdaBoostRegressor(n_estimators=50, random_state=42)\n",
        "adb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = adb_reg.predict(X_test)\n",
        "print(f\"Mean Absolute Error (MAE): {mean_absolute_error(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "76Gr8MVUNfJE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Train a Gradient Boosting Classifier on the Breast Cancer dataset and print feature importance"
      ],
      "metadata": {
        "id": "7SdwOelEDU_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "import numpy as np\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X, y)\n",
        "\n",
        "# Print feature importance\n",
        "importance = gb_clf.feature_importances_\n",
        "for i, v in enumerate(importance):\n",
        "    print(f\"Feature: {data.feature_names[i]}, Importance: {v:.2f}\")\n"
      ],
      "metadata": {
        "id": "90fRhQrZNhUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 17. Train a Gradient Boosting Regressor and evaluate using R-Squared Score"
      ],
      "metadata": {
        "id": "tqFx1fwBDU9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = gbr.predict(X_test)\n",
        "print(f\"R-Squared Score: {r2_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "eGrM4h6ZNkzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 18. Train an XGBoost Classifier on a dataset and compare accuracy with Gradient Boosting"
      ],
      "metadata": {
        "id": "wtr32aTzKdYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_clf = XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "y_pred_xgb = xgb_clf.predict(X_test)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "y_pred_gb = gb_clf.predict(X_test)\n",
        "\n",
        "# Compare accuracy\n",
        "print(f\"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb):.2f}\")\n",
        "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gb):.2f}\")\n"
      ],
      "metadata": {
        "id": "s52grZEnNqDi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 19. Train a CatBoost Classifier and evaluate using F1-Score"
      ],
      "metadata": {
        "id": "5iWRemzLKdQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a classification dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "print(f\"F1-Score: {f1_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "HdbQoR6hNsUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 20. Train an XGBoost Regressor and evaluate using Mean Squared Error (MSE)"
      ],
      "metadata": {
        "id": "efbLcMSGKdCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from xgboost import XGBRegressor\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a regression dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Regressor\n",
        "xgb_reg = XGBRegressor(n_estimators=100, random_state=42)\n",
        "xgb_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = xgb_reg.predict(X_test)\n",
        "print(f\"Mean Squared Error (MSE): {mean_squared_error(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "f9Xwm4aJNxbb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 21. Train an AdaBoost Classifier and Visualize Feature Importance"
      ],
      "metadata": {
        "id": "8cUtBE_HKc-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train AdaBoost Classifier\n",
        "adb_clf = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
        "adb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize feature importance\n",
        "importance = adb_clf.feature_importances_\n",
        "plt.bar(range(len(importance)), importance)\n",
        "plt.title('Feature Importance - AdaBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R8WMo_MnN2BU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 22. Train a Gradient Boosting Regressor and Plot Learning Curves"
      ],
      "metadata": {
        "id": "Fj3MN3h4Kc8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Plot learning curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(gbr, X, y, train_sizes=[0.1, 0.33, 0.55, 0.78, 1.0], cv=5)\n",
        "plt.plot(train_sizes, train_scores.mean(axis=1), label='Train Score')\n",
        "plt.plot(train_sizes, test_scores.mean(axis=1), label='Test Score')\n",
        "plt.title('Learning Curves - Gradient Boosting Regressor')\n",
        "plt.xlabel('Training Set Size')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6YQ_2UxMN6Sj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 23. Train an XGBoost Classifier and Visualize Feature Importance"
      ],
      "metadata": {
        "id": "rFZY2JPpKc5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import xgboost as xgb\n",
        "import matplotlib.pyplot as plt\n",
        "from xgboost import plot_importance\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Visualize feature importance\n",
        "plot_importance(xgb_clf)\n",
        "plt.title('Feature Importance - XGBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "hsnm_59FN_qH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 24. Train a CatBoost Classifier and Plot the Confusion Matrix"
      ],
      "metadata": {
        "id": "uvJBNtbpKc26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict and plot confusion matrix\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "ConfusionMatrixDisplay(cm).plot()\n",
        "plt.title('Confusion Matrix - CatBoost Classifier')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "g2NF-hYGOErG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 25. Train an AdaBoost Classifier with Different Numbers of Estimators and Compare Accuracy"
      ],
      "metadata": {
        "id": "d9Ze2LrGKc0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate AdaBoost Classifier with different numbers of estimators\n",
        "for n_estimators in [10, 50, 100]:\n",
        "    adb_clf = AdaBoostClassifier(n_estimators=n_estimators, random_state=42)\n",
        "    adb_clf.fit(X_train, y_train)\n",
        "    y_pred = adb_clf.predict(X_test)\n",
        "    print(f\"Accuracy with {n_estimators} estimators: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "0XaTf806OGmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 26. Train a Gradient Boosting Classifier and Visualize the ROC Curve"
      ],
      "metadata": {
        "id": "3I9bNCHDKcxh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = gb_clf.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.title('ROC Curve - Gradient Boosting Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "10qtsQOHOLhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 27. Train an XGBoost Regressor and Tune the Learning Rate using GridSearchCV"
      ],
      "metadata": {
        "id": "aic7y_JlKcur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameter grid for learning rate\n",
        "param_grid = {'learning_rate': [0.01, 0.1, 0.2]}\n",
        "\n",
        "# Train XGBoost Regressor with GridSearchCV\n",
        "xgb_reg = xgb.XGBRegressor(n_estimators=100, random_state=42)\n",
        "grid_search = GridSearchCV(xgb_reg, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model and evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "print(f\"Best Learning Rate: {grid_search.best_params_['learning_rate']}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "grQw6mLZOPez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 28. Train a CatBoost Classifier on an Imbalanced Dataset and Compare Performance with Class Weighting"
      ],
      "metadata": {
        "id": "4Lm5QJJ7Kcsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "# Create an imbalanced dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, weights=[0.9, 0.1], random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train CatBoost Classifier without class weights\n",
        "cat_clf = CatBoostClassifier(iterations=100, random_state=42, verbose=0)\n",
        "cat_clf.fit(X_train, y_train)\n",
        "y_pred = cat_clf.predict(X_test)\n",
        "print(f\"F1-Score without Class Weights: {f1_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Train CatBoost Classifier with class weights\n",
        "cat_clf_weighted = CatBoostClassifier(iterations=100, class_weights=[1, 10], random_state=42, verbose=0)\n",
        "cat_clf_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = cat_clf_weighted.predict(X_test)\n",
        "print(f\"F1-Score with Class Weights: {f1_score(y_test, y_pred_weighted):.2f}\")\n"
      ],
      "metadata": {
        "id": "4Mjg4JIjOSq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 29. Train an AdaBoost Classifier and Analyze the Effect of Different Learning Rates"
      ],
      "metadata": {
        "id": "IYWbBLadKcpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Create a dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train and evaluate AdaBoost Classifier with different learning rates\n",
        "for lr in [0.01, 0.1, 1]:\n",
        "    adb_clf = AdaBoostClassifier(n_estimators=50, learning_rate=lr, random_state=42)\n",
        "    adb_clf.fit(X_train, y_train)\n",
        "    y_pred = adb_clf.predict(X_test)\n",
        "    print(f\"Accuracy with learning rate {lr}: {accuracy_score(y_test, y_pred):.2f}\")\n"
      ],
      "metadata": {
        "id": "86M0cr1GOVxJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 30. Train an XGBoost Classifier for Multi-Class Classification and Evaluate using Log-Loss"
      ],
      "metadata": {
        "id": "G4JJRCm-Kcmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "# Create a multi-class dataset\n",
        "X, y = make_classification(n_samples=1000, n_features=20, n_classes=3, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train XGBoost Classifier\n",
        "xgb_clf = xgb.XGBClassifier(n_estimators=100, objective='multi:softprob', num_class=3, random_state=42)\n",
        "xgb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict probabilities and evaluate using log-loss\n",
        "y_prob = xgb_clf.predict_proba(X_test)\n",
        "print(f\"Log-Loss: {log_loss(y_test, y_prob):.2f}\")\n"
      ],
      "metadata": {
        "id": "b6NVX6thOZMs"
      }
    }
  ]
}